{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Physics-Informed Neural Network (PINN) vs Normal Neural Network\n",
        "## Simple Harmonic Motion (SHM)\n",
        "\n",
        "We solve the SHM differential equation:\n",
        "$$ \\frac{d^2y}{dt^2} + \\omega^2 y = 0 $$\n",
        "\n",
        "True solution:\n",
        "$$ y(t) = \\sin(\\omega t) $$\n",
        "\n",
        "We compare:\n",
        "1. PINN (Physics + few data points)\n",
        "2. Normal Neural Network (data only)\n",
        "\n",
        "We also compare activation functions:\n",
        "- Tanh\n",
        "- Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "omega = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "t = torch.linspace(0, 10, 200).view(-1,1).to(device)\n",
        "t.requires_grad = True\n",
        "y_true = torch.sin(omega * t)\n",
        "\n",
        "# Few data points (3 anchor points)\n",
        "t_data = torch.tensor([[0.0],[3.14],[6.28]], device=device)\n",
        "y_data = torch.sin(omega * t_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, activation='tanh'):\n",
        "        super().__init__()\n",
        "        act = nn.Tanh() if activation=='tanh' else nn.Sigmoid()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(1, 64),\n",
        "            act,\n",
        "            nn.Linear(64, 64),\n",
        "            act,\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train PINN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_pinn(activation='tanh'):\n",
        "    model = Net(activation).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    for epoch in range(3000):\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(t)\n",
        "\n",
        "        dy = torch.autograd.grad(y_pred, t, torch.ones_like(y_pred), create_graph=True)[0]\n",
        "        d2y = torch.autograd.grad(dy, t, torch.ones_like(dy), create_graph=True)[0]\n",
        "\n",
        "        physics_loss = torch.mean((d2y + omega**2 * y_pred)**2)\n",
        "        data_loss = torch.mean((model(t_data) - y_data)**2)\n",
        "\n",
        "        loss = physics_loss + data_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Normal NN (Data Only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_nn(activation='tanh'):\n",
        "    model = Net(activation).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    for epoch in range(3000):\n",
        "        optimizer.zero_grad()\n",
        "        loss = torch.mean((model(t_data) - y_data)**2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pinn_tanh = train_pinn('tanh')\n",
        "nn_tanh = train_nn('tanh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot PINN Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with torch.no_grad():\n",
        "    plt.figure()\n",
        "    plt.plot(t.cpu(), y_true.cpu(), label='Actual')\n",
        "    plt.plot(t.cpu(), pinn_tanh(t).cpu(), '--', label='PINN')\n",
        "    plt.legend()\n",
        "    plt.xlabel('t')\n",
        "    plt.ylabel('y')\n",
        "    plt.title('PINN (Tanh Activation)')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Normal NN Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with torch.no_grad():\n",
        "    plt.figure()\n",
        "    plt.plot(t.cpu(), y_true.cpu(), label='Actual')\n",
        "    plt.plot(t.cpu(), nn_tanh(t).cpu(), '--', label='Standard NN')\n",
        "    plt.legend()\n",
        "    plt.xlabel('t')\n",
        "    plt.ylabel('y')\n",
        "    plt.title('Normal NN (Tanh Activation)')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observations\n",
        "- PINN follows oscillatory physics even with very few data points.\n",
        "- Standard NN overfits anchor points and fails to generalize.\n",
        "- Tanh typically performs better than Sigmoid due to zero-centered gradients.\n",
        "- Physics constraint dramatically improves extrapolation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}